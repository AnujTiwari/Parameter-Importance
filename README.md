Parameter Importance Assessment
Modern data sets are often described with far too many variables for practical model building. Usually most of these variables are irrelevant to the predictive modeling, and obviously their relevance is not known in advance. There are several disadvantages of dealing with overlarge feature sets. One is purely technical — dealing with large feature sets slows down algorithms, takes too many resources and is simply inconvenient. Another is even more important — many machine learning algorithms exhibit a decrease of accuracy when the number of variables is significantly higher than optimal. Therefore selection of the small (possibly optimal) feature set ensure best possible predictive modeking results is desirable for practical reasons.

In this repository, you will get R code for implementing four paramter importance assessment techniques which are based on some of the most popular machine learning and statistical modeling algorithms. 

1. Random Forest - Boruta
2. Decision Tree/ Logistic Regression/ Linear Regression - Recursive Feature Elimination (RFE)
3. Decision Tree - Recursive PARTitioning (RPART)
4. Linear Regression - LMG

